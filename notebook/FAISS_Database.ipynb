{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ce978f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Introduction to FAISS\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors.  \n",
    "It is particularly powerful for:\n",
    "\n",
    "- **Large-scale vector search**: Can handle millions or even billions of vectors  \n",
    "- **Multiple index types**: Ranges from brute force to approximate methods  \n",
    "- **GPU acceleration**: Provides significant speedup for large datasets  \n",
    "- **Memory efficiency**: Offers various compression techniques to save memory  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "333d4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a17da",
   "metadata": {},
   "source": [
    "### Chunking Your Resume by section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31228c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sections: 8\n",
      "\n",
      "Section 1 (CLEAVESTONE ADUNGO ):\n",
      "CLEAVESTONE ADUNGO \n",
      "cleavestone94@gmail.com   |   +254703457427   |   Nairobi Kenya       \n",
      "GitHub: Link  |   Linkedln: Link | Portfolio: Link...\n",
      "\n",
      "Section 2 (Summary      ):\n",
      "Summary      \n",
      "Data Scientist transitioning from finance with 2+ years of hands-on ML, NLP, and data analytics experience. \n",
      "Skilled in Python, SQL, statistical modeling, LLM fine-tuning, RAG systems, and MLOps pipelines. Diverse \n",
      "industry background (education, logistics, finance, data annotation) wi...\n",
      "\n",
      "Section 3 (Skills      ):\n",
      "Skills      \n",
      "Programming & Analytics: Python (pandas, NumPy, scikit-learn, matplotlib, seaborn), SQL, Excel \n",
      "Machine Learning & AI: Classification, Regression, NLP, Deep Learning, Feature Engineering, Model Evaluation \n",
      "LLMs & Conversational AI: LangChain, RAG, Vector Databases (Pinecone, LanceDB), P...\n",
      "\n",
      "Section 4 (Skills: Analytical Thinking, Problem-Solving, Communication, Self-Directed Learning ):\n",
      "Skills: Analytical Thinking, Problem-Solving, Communication, Self-Directed Learning...\n",
      "\n",
      "Section 5 (Experience      ):\n",
      "Experience      \n",
      " \n",
      "Zindi Africa – Data Science Competitions April 2025 – Present \n",
      " Participated in multiple machine learning competitions tackling real-world problems in healthcare and agriculture. \n",
      " Applied methods such as classification, regression, feature engineering, and LLM fine-tuning (LoRA...\n",
      "\n",
      "Section 6 (Education and Training      ):\n",
      "Education and Training      \n",
      "Moi University – Eldoret, Kenya \n",
      "Bachelor of Science in Mathematics | 2012 – 2017 \n",
      "Second Class Honors, Upper Division \n",
      "Relevant coursework: Probability & Statistics, Linear Algebra, Real Analysis, Numerical Methods, \n",
      "Mathematical Modeling, Calculus...\n",
      "\n",
      "Section 7 (Projects      ):\n",
      "Projects      \n",
      "#HRJ#784bc4ba-1ee5-4196-8c02-14eda4a9e2f9# \n",
      "Customer Churn Prediction – End-to-End MLOps Pipeline \n",
      "Built a production-ready churn prediction system for a banking dataset using LightGBM, achieving an ROC AUC of 0.86. \n",
      "Designed a modular ML pipeline (data ingestion → preprocessing → tra...\n",
      "\n",
      "Section 8 (Certifications      ):\n",
      "Certifications      \n",
      "• Certificate in MLOPS from Udemy \n",
      "• Certificate in LLM Engineering from Udemy \n",
      "• Certificate in Foundational Generative AI from Ineuron...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load the PDF\n",
    "pdf_path = \"../data/resume.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "\n",
    "# Combine text\n",
    "cv_text = \"\".join(page.page_content for page in pages)\n",
    "\n",
    "# Define section-based splitting using regex\n",
    "sections = re.split(r\"(?=Summary|Skills|Experience|Education and Training|Projects|Certifications)\", cv_text)\n",
    "\n",
    "# Create Documents for each section\n",
    "cv_documents = [\n",
    "    Document(page_content=section.strip(), metadata={\"source\": \"cv\", \"section\": section.split(\"\\n\",1)[0]})\n",
    "    for section in sections if section.strip()\n",
    "]\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of sections: {len(cv_documents)}\")\n",
    "for i, doc in enumerate(cv_documents):\n",
    "    print(f\"\\nSection {i+1} ({doc.metadata['section']}):\\n{doc.page_content[:300]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37d1cab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certifications      \\n• Certificate in MLOPS from Udemy \\n• Certificate in LLM Engineering from Udemy \\n• Certificate in Foundational Generative AI from Ineuron'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_documents[7].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c440c2",
   "metadata": {},
   "source": [
    "### Generating Embeddings with Sentence-BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "936d01d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (8, 384)\n",
      "Sample embedding (first 5 dims): [-0.05490848  0.05856662 -0.03859244  0.03439187 -0.01320006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract plain text from Documents\n",
    "texts = [doc.page_content for doc in cv_documents]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # (n_chunks, 384)\n",
    "print(f\"Sample embedding (first 5 dims): {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f1446",
   "metadata": {},
   "source": [
    "### Building and Using the FAISS \n",
    "IndexFAISS indexes vectors for O(log n) searches vs. O(n) brute-force.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975a3d7",
   "metadata": {},
   "source": [
    "#### Simple Exact Search (IndexFlatL2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b791f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors indexed: 8\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "d = embeddings.shape[1]  # 384\n",
    "index = faiss.IndexFlatL2(d)  # Exact L2 search\n",
    "\n",
    "# Add embeddings\n",
    "index.add(embeddings.astype('float32'))  # FAISS needs float32\n",
    "print(f\"Total vectors indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c2fd9",
   "metadata": {},
   "source": [
    "#### Approximate Search (IndexIVFFlat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f91972b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 8 vectors with 2 clusters\n"
     ]
    }
   ],
   "source": [
    "# Parameters: nlist = sqrt(n_vectors) for balance\n",
    "nlist = int(np.sqrt(len(texts)))  # e.g., 2 for 6 chunks\n",
    "quantizer = faiss.IndexFlatL2(d)  # Coarse quantizer\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "\n",
    "# Train on embeddings (needs > nlist vectors; for small data, use all)\n",
    "index.train(embeddings.astype('float32'))\n",
    "\n",
    "# Add vectors\n",
    "index.add(embeddings.astype('float32'))\n",
    "print(f\"Indexed {index.ntotal} vectors with {nlist} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfb5c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 clusters for 8 chunks\n",
      "Indexed 8 vectors with 2 clusters\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(embeddings)  # Ensures unit length for cosine\n",
    "\n",
    "# Set number of clusters (nlist)\n",
    "# For small datasets (<10 chunks), use nlist=1 or min(4, len(texts))\n",
    "nlist = max(1, min(4, int(np.sqrt(len(texts)))))  # Avoid too few clusters\n",
    "print(f\"Using {nlist} clusters for {len(texts)} chunks\")\n",
    "\n",
    "# Create quantizer with cosine similarity (Inner Product)\n",
    "d = embeddings.shape[1]  # 384 dimensions\n",
    "quantizer = faiss.IndexFlatIP(d)  # Use IP for cosine similarity\n",
    "\n",
    "# Initialize IndexIVFFlat\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# Train on embeddings (needs > nlist vectors; for small data, use all)\n",
    "index.train(embeddings.astype('float32'))\n",
    "\n",
    "# Add vectors to index\n",
    "index.add(embeddings.astype('float32'))\n",
    "print(f\"Indexed {index.ntotal} vectors with {nlist} clusters\")\n",
    "\n",
    "# Optional: Set nprobe for search accuracy (higher = more accurate, slower)\n",
    "index.nprobe = min(nlist, 2)  # For small datasets, search 1-2 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39117513",
   "metadata": {},
   "source": [
    "#### Performing Similarity Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5e3c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Score 0.730 | Chunk: page_content='Summary      \n",
      "Data Scientist transitioning from finance with 2+ years of hands-on ML, NLP, and data analytics experience.' metadata={'source': 'cv'}\n",
      "Rank 2: Score 0.837 | Chunk: page_content='industry background (education, logistics, finance, data annotation) with a proven track record of turning raw' metadata={'source': 'cv'}\n",
      "Rank 3: Score 0.840 | Chunk: page_content='Skilled in Python, SQL, statistical modeling, LLM fine-tuning, RAG systems, and MLOps pipelines. Diverse' metadata={'source': 'cv'}\n"
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "query= \"experience in finance\"\n",
    "query_embedding = model.encode([query]).astype('float32')\n",
    "\n",
    "# Search: k=3 nearest neighbors, returns distances + indices\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Results\n",
    "for i in range(k):\n",
    "    chunk_idx = indices[0][i]\n",
    "    similarity = 1 - (distances[0][i] / 2)  # Convert L2 to approx cosine (if normalized)\n",
    "    print(f\"Rank {i+1}: Score {similarity:.3f} | Chunk: {chunks[chunk_idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
