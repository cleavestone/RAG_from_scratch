{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ad3584",
   "metadata": {},
   "source": [
    "## üîπ Introduction to ANN Algorithms  \n",
    "\n",
    "**Approximate Nearest Neighbors (ANN)** algorithms are designed to find vectors in a **high-dimensional space** that are close to a query vector, based on a distance metric (e.g., **Euclidean**, **Cosine**).  \n",
    "\n",
    "Unlike exact nearest neighbor search, ANN sacrifices some **accuracy** for significant **speed improvements**, making it ideal for **large-scale applications** like **RAG**, where embeddings (e.g., from BERT or other LLMs) need to be searched quickly.  \n",
    "\n",
    "In **RAG**, ANN is used to retrieve relevant documents or embeddings from a **vector database** to **augment the generation process**.  \n",
    "\n",
    "### ‚úÖ Popular ANN Algorithms  \n",
    "\n",
    "- **HNSW (Hierarchical Navigable Small World)**  \n",
    "  - Graph-based method.  \n",
    "  - Builds a **multi-layered structure** for fast and accurate searches.  \n",
    "\n",
    "- **IVF (Inverted File Index)**  \n",
    "  - Quantization-based method.  \n",
    "  - **Clusters vectors** and searches within the closest clusters.  \n",
    "\n",
    "- **LSH (Locality-Sensitive Hashing)**  \n",
    "  - Hashing-based method.  \n",
    "  - Maps **similar vectors** to the **same hash buckets** for fast retrieval.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8eb8c",
   "metadata": {},
   "source": [
    "## üîπ HNSW (Hierarchical Navigable Small World)  \n",
    "\n",
    "### 3.1 Theory  \n",
    "\n",
    "**HNSW** is a **graph-based ANN algorithm** that builds a hierarchical structure of interconnected nodes (vectors) to enable **fast searches**.  \n",
    "\n",
    "It is based on the **Navigable Small World (NSW)** graph, where each node is connected to a **small set of neighbors**, ensuring short paths between any two nodes.  \n",
    "\n",
    "---\n",
    "\n",
    "### üìê Structure  \n",
    "- Organizes vectors into **multiple layers**.  \n",
    "- Higher layers ‚Üí **sparse** (fewer nodes).  \n",
    "- Lower layers ‚Üí **denser**.  \n",
    "- The **top layer** contains only a few nodes, while the **bottom layer** contains *all vectors*.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîé Search Process  \n",
    "1. **Start at the top layer** and find the closest node to the query.  \n",
    "2. **Move to the next layer**, using the previous layer‚Äôs result as the entry point.  \n",
    "3. **Repeat until the bottom layer**, refining the neighbor list at each step.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7747caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Brute Force Search\n",
      "Top 5 neighbors: [ 7461 30543 35161 18054 33682]\n",
      "Distances: [3.6132355 3.6199749 3.6956348 3.7309732 3.7433467]\n",
      "Time taken: 0.0631 seconds\n",
      "\n",
      "‚ö° HNSW Search\n",
      "Top 5 neighbors: [35161 15105 38889  8386  3244]\n",
      "Distances: [13.657718 14.171591 14.38143  14.567127 14.903164]\n",
      "Time taken: 0.000000 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hnswlib\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import time\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Generate synthetic embeddings\n",
    "# -------------------------\n",
    "np.random.seed(42)\n",
    "dimension = 128          # Embedding dimension\n",
    "num_elements = 50000     # Number of \"documents\"\n",
    "data = np.random.random((num_elements, dimension)).astype('float32')\n",
    "\n",
    "# Create a query vector\n",
    "query = np.random.random((1, dimension)).astype('float32')\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Brute force search (for comparison)\n",
    "# -------------------------\n",
    "start = time.time()\n",
    "distances = euclidean_distances(query, data)[0]\n",
    "top_k = np.argsort(distances)[:5]\n",
    "brute_force_time = time.time() - start\n",
    "\n",
    "print(\"üîé Brute Force Search\")\n",
    "print(f\"Top 5 neighbors: {top_k}\")\n",
    "print(f\"Distances: {distances[top_k]}\")\n",
    "print(f\"Time taken: {brute_force_time:.4f} seconds\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: HNSW Indexing\n",
    "# -------------------------\n",
    "index = hnswlib.Index(space='l2', dim=dimension)  # 'l2' = Euclidean distance\n",
    "index.init_index(max_elements=num_elements, ef_construction=200, M=16)\n",
    "\n",
    "# Add items to index\n",
    "index.add_items(data, np.arange(num_elements))\n",
    "\n",
    "# ef controls accuracy/speed trade-off\n",
    "index.set_ef(50)\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: HNSW Search\n",
    "# -------------------------\n",
    "start = time.time()\n",
    "labels, distances = index.knn_query(query, k=5)\n",
    "hnsw_time = time.time() - start\n",
    "\n",
    "print(\"‚ö° HNSW Search\")\n",
    "print(f\"Top 5 neighbors: {labels[0]}\")\n",
    "print(f\"Distances: {distances[0]}\")\n",
    "print(f\"Time taken: {hnsw_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f5603",
   "metadata": {},
   "source": [
    "## üîπ IVF (Inverted File Index)  \n",
    "\n",
    "### 4.1 Theory  \n",
    "\n",
    "**IVF** is a **quantization-based ANN algorithm** that partitions the vector space into **clusters** using a clustering algorithm (e.g., **k-means**).  \n",
    "Each vector is assigned to the **nearest cluster centroid**, and search is performed within a **subset of clusters** instead of the entire dataset.  \n",
    "\n",
    "---\n",
    "\n",
    "### üìê Structure  \n",
    "- **Codebook**: A set of cluster centroids (learned via k-means).  \n",
    "- **Inverted Lists**: Each centroid points to a list of vectors assigned to that cluster.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîé Search Process  \n",
    "1. **Find the `nprobe` closest centroids** to the query vector.  \n",
    "2. **Search for nearest neighbors** within the inverted lists of those centroids.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3830fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 nearest neighbors (indices): [8769 9571 4436 1056  556]\n",
      "Distances: [13.34696   14.837141  15.379183  15.488777  15.4977865]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "dimension = 128\n",
    "num_elements = 10000\n",
    "data = np.random.random((num_elements, dimension)).astype('float32')\n",
    "query = np.random.random((1, dimension)).astype('float32')\n",
    "\n",
    "# Train the IVF index\n",
    "nlist = 100  # Number of clusters\n",
    "quantizer = faiss.IndexFlatL2(dimension)  # Flat index for quantizer\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)\n",
    "index.train(data)  # Train the clustering model\n",
    "index.add(data)    # Add vectors to the index\n",
    "\n",
    "# Set number of clusters to probe during search\n",
    "index.nprobe = 10\n",
    "\n",
    "# Query the index for top-5 nearest neighbors\n",
    "k = 5\n",
    "distances, labels = index.search(query, k)\n",
    "\n",
    "print(f\"Top {k} nearest neighbors (indices): {labels[0]}\")\n",
    "print(f\"Distances: {distances[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859f5cd",
   "metadata": {},
   "source": [
    "## üîπ LSH (Locality-Sensitive Hashing)  \n",
    "\n",
    "### 5.1 Theory  \n",
    "\n",
    "**LSH** is a **hashing-based ANN algorithm** that maps similar vectors to the **same hash buckets** with high probability.  \n",
    "It uses **hash functions designed to preserve locality**, meaning similar vectors produce similar hashes.  \n",
    "\n",
    "---\n",
    "\n",
    "### üìê Structure  \n",
    "- **Hash Functions**: Random projections or other locality-sensitive functions that map vectors to buckets.  \n",
    "- **Hash Tables**: Multiple hash tables store vectors, increasing the chance of finding neighbors.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîé Search Process  \n",
    "1. **Hash the query vector** to identify candidate buckets.  \n",
    "2. **Search for nearest neighbors** within those buckets.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c487a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 nearest neighbors (indices): [2980 4436 5238 7666 1032]\n",
      "Distances: [92. 92. 92. 92. 94.]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "dimension = 128\n",
    "num_elements = 10000\n",
    "data = np.random.random((num_elements, dimension)).astype('float32')\n",
    "query = np.random.random((1, dimension)).astype('float32')\n",
    "\n",
    "# Initialize LSH index\n",
    "nbits = dimension * 4  # Number of bits for hashing\n",
    "index = faiss.IndexLSH(dimension, nbits)\n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(data)\n",
    "\n",
    "# Query the index for top-5 nearest neighbors\n",
    "k = 5\n",
    "distances, labels = index.search(query, k)\n",
    "\n",
    "print(f\"Top {k} nearest neighbors (indices): {labels[0]}\")\n",
    "print(f\"Distances: {distances[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9854a69",
   "metadata": {},
   "source": [
    "## üîπ Comparing HNSW, IVF, and LSH  \n",
    "\n",
    "| Algorithm | Type              | Accuracy | Speed      | Memory | Best Use Case in RAG                          |\n",
    "|-----------|-------------------|----------|------------|--------|-----------------------------------------------|\n",
    "| **HNSW**  | Graph-based       | High     | Fast       | High   | High-accuracy retrieval, moderate datasets    |\n",
    "| **IVF**   | Quantization-based| Medium   | Fast       | Medium | Large datasets, balanced speed/accuracy       |\n",
    "| **LSH**   | Hashing-based     | Low      | Very Fast  | Low    | Very large datasets, coarse retrieval         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251b4bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hnswlib\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: Generate document and query embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Germany is known for its engineering and automotive industries.\",\n",
    "    \"Japan has a rich cultural heritage, including tea ceremonies and samurai traditions.\"\n",
    "]\n",
    "doc_embeddings = np.vstack([get_embedding(doc) for doc in documents])\n",
    "\n",
    "# Step 2: Build HNSW index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = hnswlib.Index(space='cosine', dim=dimension)\n",
    "index.init_index(max_elements=len(documents), ef_construction=200, M=16)\n",
    "index.add_items(doc_embeddings, np.arange(len(documents)))\n",
    "index.set_ef(50)\n",
    "\n",
    "# Step 3: Query the index\n",
    "query = \"What is the capital of France?\"\n",
    "query_embedding = get_embedding(query)\n",
    "k = 1\n",
    "labels, distances = index.knn_query(query_embedding, k=k)\n",
    "\n",
    "# Step 4: Retrieve documents and generate response\n",
    "retrieved_doc = documents[labels[0][0]]\n",
    "generator = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "prompt = f\"Context: {retrieved_doc}\\nQuestion: {query}\\nAnswer:\"\n",
    "inputs = gen_tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = generator.generate(**inputs, max_length=50)\n",
    "answer = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Retrieved Document: {retrieved_doc}\")\n",
    "print(f\"Generated Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
