# ðŸ“š Retrieval-Augmented Generation (RAG) Learning Journey

This repository documents my hands-on learning path in **Retrieval-Augmented Generation (RAG)**.  
The goal is to build a strong foundation, explore different components step by step, and eventually design scalable RAG systems for real-world use cases.  

---

## âœ… Completed Notebooks

### 1. Mathematical Foundations of RAG
- Understanding the **core math** behind retrieval and generation.  
- Probability basics, similarity metrics, and vector space concepts.  
- Provides the groundwork for later modules on embeddings and retrieval.  

### 2. Text Chunking Strategies
- Explored why **chunking is critical** for RAG.  
- Implemented and compared multiple strategies:  
  - Fixed-size chunking  
  - Sentence-based chunking  
  - Semantic chunking  
  - Overlapping windows  
  - Structure-aware chunking  

### 3. Embedding Models  
- Learned about **Word embeddings, sentence embeddings, document embeddings, and multilingual embeddings**.  
- Hands-on with pre-trained models like `all-MiniLM-L6-v2` (SBERT).  
- Implemented **cosine similarity** to compare embeddings.  
- Covered **fine-tuning embeddings** for domain-specific tasks.  

### 4. Approximate Nearest Neighbor (ANN) Algorithms
- Studied and implemented **ANN algorithms** for efficient similarity search.  
- Explored three major approaches:  
  - **HNSW (Hierarchical Navigable Small World)** â†’ Graph-based, high accuracy.  
  - **IVF (Inverted File Index)** â†’ Quantization-based, efficient on large datasets.  
  - **LSH (Locality-Sensitive Hashing)** â†’ Hashing-based, very fast but coarse.  
- Compared trade-offs in **accuracy, speed, memory, and RAG use cases**.

